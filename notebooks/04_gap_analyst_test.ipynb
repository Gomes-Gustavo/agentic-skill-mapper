{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Gap Analyst Agent - Advanced Normalization\n",
    "\n",
    "### Objective\n",
    "This notebook develops and tests the `Gap Analyst` agent. A key feature of this notebook is the implementation of an advanced semantic normalization pipeline. Instead of relying on manual rules, we will use vector embeddings and community detection (a form of clustering) to intelligently group and standardize the skills extracted by our `Market Analyst` agent. This creates a highly accurate and scalable foundation for the gap analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from typing import List, Set\n",
    "\n",
    "# Load the processed data from Notebook 2\n",
    "processed_file_path = '../data/processed/processed_google_jobs.csv'\n",
    "df = pd.read_csv(processed_file_path)\n",
    "\n",
    "# Convert string-lists back to actual list objects\n",
    "def safe_converter(value):\n",
    "    try: return ast.literal_eval(value)\n",
    "    except: return []\n",
    "\n",
    "df['extracted_technical_skills'] = df['extracted_technical_skills'].apply(safe_converter)\n",
    "df['extracted_soft_skills'] = df['extracted_soft_skills'].apply(safe_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Skill Aggregation for a Target Role\n",
    "\n",
    "Before we can normalize the skills, we first need to collect them. We'll choose a target job category (e.g., 'Software Engineering') and aggregate all the raw skill strings mentioned in those job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 278 total skill instances.\n",
      "Found 110 unique skills to normalize.\n"
     ]
    }
   ],
   "source": [
    "target_category = 'Software Engineering'\n",
    "\n",
    "df_filtered = df[df['Category'].str.contains(target_category, case=False, na=False)]\n",
    "\n",
    "tech_skills = [skill for sublist in df_filtered['extracted_technical_skills'] for skill in sublist if skill]\n",
    "soft_skills = [skill for sublist in df_filtered['extracted_soft_skills'] for skill in sublist if skill]\n",
    "all_skills_raw = tech_skills + soft_skills\n",
    "\n",
    "unique_skills = sorted(list(set(skill.strip().lower() for skill in all_skills_raw)))\n",
    "\n",
    "print(f\"Found {len(all_skills_raw)} total skill instances.\")\n",
    "print(f\"Found {len(unique_skills)} unique skills to normalize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Normalization via Embeddings and Clustering\n",
    "\n",
    "This is the core of our intelligent normalization pipeline. We will process our list of `unique_skills` in three steps:\n",
    "1.  **Generate Embeddings**: Convert each skill string into a meaningful numerical vector using a pre-trained `SentenceTransformer` model.\n",
    "2.  **Cluster Skills**: Use a community detection algorithm to group vectors (and thus skills) that are semantically similar.\n",
    "3.  **Generate Normalization Map**: Create a mapping dictionary from these clusters to standardize skill variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model...\n",
      "Model loaded.\n",
      "Embeddings generated. Shape: torch.Size([110, 384])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "skill_embeddings = model.encode(unique_skills, convert_to_tensor=True)\n",
    "print(f\"Embeddings generated. Shape: {skill_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering similar skills...\n",
      "Clustering complete. Found 5 skill clusters.\n",
      "\n",
      "--- Sample Skill Clusters ---\n",
      "Cluster 1: ['unix/linux', 'unix', 'linux']\n",
      "Cluster 2: ['c++', 'c/c++']\n",
      "Cluster 3: ['code review', 'code reviews']\n",
      "Cluster 4: ['objective c', 'objective-c']\n",
      "Cluster 5: ['problem solving', 'problem-solving']\n"
     ]
    }
   ],
   "source": [
    "# We use a community detection algorithm to find clusters of similar skills.\n",
    "# `min_community_size` groups skills only if a cluster has at least 2 members.\n",
    "\n",
    "print(\"Clustering similar skills...\")\n",
    "clusters = util.community_detection(skill_embeddings, min_community_size=2, threshold=0.85)\n",
    "print(f\"Clustering complete. Found {len(clusters)} skill clusters.\")\n",
    "\n",
    "print(\"\\n--- Sample Skill Clusters ---\")\n",
    "for i, cluster in enumerate(clusters[:10]):\n",
    "    print(f\"Cluster {i+1}: {[unique_skills[skill_id] for skill_id in cluster]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated a normalization map with 11 entries.\n",
      "\n",
      "--- Sample of the generated map ---\n",
      "'unix/linux'  ->  'unix'\n",
      "'unix'  ->  'unix'\n",
      "'linux'  ->  'unix'\n",
      "'c++'  ->  'c++'\n",
      "'c/c++'  ->  'c++'\n",
      "'code review'  ->  'code review'\n",
      "'code reviews'  ->  'code review'\n",
      "'objective c'  ->  'objective c'\n",
      "'objective-c'  ->  'objective c'\n",
      "'problem solving'  ->  'problem solving'\n"
     ]
    }
   ],
   "source": [
    "# Now, we create our normalization map automatically from the clusters.\n",
    "skill_mapping = {}\n",
    "for cluster in clusters:\n",
    "\n",
    "    cluster_skills = [unique_skills[skill_id] for skill_id in cluster]\n",
    "    \n",
    "    # Choose the shortest name as the standard for the group\n",
    "    canonical_name = min(cluster_skills, key=len)\n",
    "    \n",
    "    # Map all skills in the cluster to the canonical name\n",
    "    for skill in cluster_skills:\n",
    "        skill_mapping[skill] = canonical_name\n",
    "\n",
    "print(f\"Generated a normalization map with {len(skill_mapping)} entries.\")\n",
    "print(\"\\n--- Sample of the generated map ---\")\n",
    "for i, (key, value) in enumerate(skill_mapping.items()):\n",
    "    if i >= 10: break\n",
    "    print(f\"'{key}'  ->  '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 30 Most In-Demand Market Skills (Semantically Normalized) ---\n",
      "['problem solving', 'c++', 'communication', 'python', 'teamwork', 'java', 'agile methodologies', 'javascript', 'c', 'go', 'c#', 'unix', 'objective c', 'collaboration', 'leadership', 'rtl', 'code review', 'stratus', 'perl', 'shell', 'systemc', 'machine learning', 'catapult', 'vivado', 'vp9', 'angularjs', 'sql', 'css', 'unit testing', 'software design']\n"
     ]
    }
   ],
   "source": [
    "# Apply the basic normalization (lowercase, strip) first\n",
    "normalized_skills = [skill.strip().lower() for skill in all_skills_raw]\n",
    "\n",
    "# Apply the advanced semantic normalization using our auto-generated map\n",
    "final_skills = [skill_mapping.get(skill, skill) for skill in normalized_skills]\n",
    "\n",
    "# Now, we calculate the frequencies on the fully cleaned and normalized data\n",
    "SKILL_BENCHMARK_COUNT = 30\n",
    "market_skills_series = pd.Series(final_skills)\n",
    "top_market_skills = market_skills_series.value_counts().head(SKILL_BENCHMARK_COUNT)\n",
    "\n",
    "\n",
    "market_skills_list = top_market_skills.index.tolist()\n",
    "\n",
    "print(f\"\\n--- Top {SKILL_BENCHMARK_COUNT} Most In-Demand Market Skills (Semantically Normalized) ---\")\n",
    "print(market_skills_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining the User's Current Skills (Simulation)\n",
    "\n",
    "With the market skill benchmark established, we now need the second piece of input for our `Gap Analyst`: a list of skills the user already possesses. In a real application, this would be provided dynamically. For our development and testing, we will simulate it with a predefined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulated User Skills ---\n",
      "['python', 'sql', 'java', 'git', 'data structures', 'algorithms', 'debugging', 'problem-solving', 'communication', 'teamwork', 'scrum', 'api design']\n",
      "\n",
      "The simulated user has 12 skills.\n"
     ]
    }
   ],
   "source": [
    "user_skills_list = [\n",
    "    'python',\n",
    "    'sql',\n",
    "    'java',\n",
    "    'git',\n",
    "    'data structures',\n",
    "    'algorithms',\n",
    "    'debugging',\n",
    "    'problem-solving',\n",
    "    'communication',\n",
    "    'teamwork',\n",
    "    'scrum', \n",
    "    'api design' \n",
    "]\n",
    "\n",
    "print(\"--- Simulated User Skills ---\")\n",
    "print(user_skills_list)\n",
    "print(f\"\\nThe simulated user has {len(user_skills_list)} skills.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing and Running the Gap Analyst\n",
    "\n",
    "With both the market and user skill lists prepared, we can now define and execute the core logic of the `Gap Analyst`. The function will be simple, robust, and efficient, using Python sets to perform the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis Complete ---\n",
      "Target Role: 'Software Engineering'\n",
      "Total Market Skills in Benchmark: 30\n",
      "User's Current Skills: 12\n",
      "---------------------------------\n",
      "\n",
      "Identified 25 Skill Gaps to focus on:\n",
      "- Agile Methodologies\n",
      "- Angularjs\n",
      "- C\n",
      "- C#\n",
      "- C++\n",
      "- Catapult\n",
      "- Code Review\n",
      "- Collaboration\n",
      "- Css\n",
      "- Go\n",
      "- Javascript\n",
      "- Leadership\n",
      "- Machine Learning\n",
      "- Objective C\n",
      "- Perl\n",
      "- Problem Solving\n",
      "- Rtl\n",
      "- Shell\n",
      "- Software Design\n",
      "- Stratus\n",
      "- Systemc\n",
      "- Unit Testing\n",
      "- Unix\n",
      "- Vivado\n",
      "- Vp9\n"
     ]
    }
   ],
   "source": [
    "def find_skill_gaps(market_skills, user_skills):\n",
    "    \"\"\"\n",
    "    Compares market skills with user skills to find the gaps.\n",
    "    \"\"\"\n",
    "    # Normalize inputs and convert to sets for efficient difference calculation\n",
    "    market_set = {skill.lower().strip() for skill in market_skills}\n",
    "    user_set = {skill.lower().strip() for skill in user_skills}\n",
    "    \n",
    "    # Calculate the skills that are in the market set but not in the user's set\n",
    "    gap_set = market_set.difference(user_set)\n",
    "    \n",
    "    # Return as a sorted list for consistent and readable output\n",
    "    return sorted(list(gap_set))\n",
    "\n",
    "\n",
    "skill_gaps = find_skill_gaps(market_skills_list, user_skills_list)\n",
    "\n",
    "print(f\"\\n--- Analysis Complete ---\")\n",
    "print(f\"Target Role: '{target_category}'\")\n",
    "print(f\"Total Market Skills in Benchmark: {len(market_skills_list)}\")\n",
    "print(f\"User's Current Skills: {len(user_skills_list)}\")\n",
    "print(f\"---------------------------------\")\n",
    "print(f\"\\nIdentified {len(skill_gaps)} Skill Gaps to focus on:\")\n",
    "\n",
    "\n",
    "for skill in skill_gaps:\n",
    "    print(f\"- {skill.title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Modularization\n",
    "\n",
    "This development notebook successfully achieved its objectives. We have:\n",
    "\n",
    "1.  **Developed `get_top_skills_by_category`**: A sophisticated function was defined and tested directly within this notebook, creating a dynamic skill benchmark using an advanced semantic normalization pipeline.\n",
    "2.  **Developed `find_skill_gaps`**: The core logic for the `Gap Analyst` agent was defined and validated, accurately identifying missing skills.\n",
    "\n",
    "### Next Step: Creating the Application Modules\n",
    "\n",
    "With the core logic for Phase 2 now validated, the final step is to create the permanent application modules. **These new `.py` files will be created with the final, validated code developed within this notebook.**\n",
    "\n",
    "-   **`src/core/skill_processing.py`**: This file will contain the `get_top_skills_by_category` function, including the complete semantic normalization pipeline.\n",
    "-   **`src/agents/gap_analyst.py`**: This file will contain the `find_skill_gaps` function, which forms the core logic of our second agent.\n",
    "\n",
    "This action cleanly separates our experimental and development work (this notebook) from our final, organized application code (the `.py` modules)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
